import pandas as pd
import xgboost as xgb
import optuna
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

from pack_series import preprocess_and_flatten_with_soh

# Load and preprocess data
df = pd.read_csv('Datasets/isu_Interp_flat.csv')
final_df = preprocess_and_flatten_with_soh(df)

# Define group split
train_groups = ['G1', 'G2', 'G3', 'G4', 'G5', 'G6']
test_groups = ['G7', 'G8', 'G9']

# Ensure group_id is string
final_df['group_id'] = final_df['group_id'].astype(str)

# Split into train/test based on group_id
train_df = final_df[final_df['group_id'].isin(train_groups)].copy()
test_df = final_df[final_df['group_id'].isin(test_groups)].copy()

# Define columns
feature_cols = final_df.columns[:-1]
target_col = final_df.columns[-1]
categorical_cols = ['group_id', 'rpt_index']

# Separate X/y
X_train, y_train = train_df[feature_cols], train_df[target_col]
X_test, y_test = test_df[feature_cols], test_df[target_col]

# Process categorical columns
for col in categorical_cols:
    X_train[col] = X_train[col].astype('category')
    X_test[col] = X_test[col].astype('category')

# Standardize numeric columns
numeric_cols = X_train.select_dtypes(include='number').columns.difference(categorical_cols)
scaler = StandardScaler()
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

# Encode categoricals as integers
for col in categorical_cols:
    X_train[col] = X_train[col].cat.codes
    X_test[col] = X_test[col].cat.codes

# -------------------------------
# ✅ OPTUNA OBJECTIVE FUNCTION
# -------------------------------
def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'random_state': 42,
    }

    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, verbose=False)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse  # minimize MSE

# -------------------------------
# ✅ RUN OPTUNA STUDY
# -------------------------------
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50, show_progress_bar=True)

print("Best trial:")
print(f"  MSE: {study.best_value:.4f}")
print("  Params:", study.best_params)

# -------------------------------
# ✅ FINAL MODEL WITH BEST PARAMS
# -------------------------------
best_model = xgb.XGBRegressor(**study.best_params)
best_model.fit(X_train, y_train)

y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nFinal Evaluation on Test Set:")
print(f"Mean Squared Error: {mse:.4f}")
print(f"R² Score: {r2:.4f}")
